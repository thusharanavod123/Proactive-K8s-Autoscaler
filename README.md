
---# ML-DevOps Workload Predictor



This project implements a proactive auto-scaling framework using LSTM neural networks to predict Kubernetes workloads and optimize resource allocation.



1ï¸âƒ£ GAP ANALYSIS

â”œâ”€â”€ Systematic literature review (40+ papers)

â”œâ”€â”€ Identify reactive vs proactive gaps

â””â”€â”€ Confirm: No pre-deployment prediction frameworks



2ï¸âƒ£ FRAMEWORK DESIGN

â”œâ”€â”€ ML-Driven DevOps Architecture

â”œâ”€â”€ Jenkins â†’ ML Prediction â†’ K8s Auto-scaling

â””â”€â”€ Modules: Workload Predictor + Resource Optimizer



3ï¸âƒ£ ML MODEL TRAINING

â”œâ”€â”€ LSTM (time-series workload prediction)

â”œâ”€â”€ Isolation Forest (anomaly detection)

â”œâ”€â”€ Dataset: Google Cluster Data 2019 (2GB public)

â””â”€â”€ Target: 85% accuracy, RMSE < 0.1



4ï¸âƒ£ PROTOTYPE IMPLEMENTATION

â”œâ”€â”€ Jenkins CI/CD pipeline with ML webhook

â”œâ”€â”€ Minikube/Kubernetes deployment

â”œâ”€â”€ AWS Free Tier EC2 hosting

â”œâ”€â”€ Prometheus + Grafana monitoring

â””â”€â”€ Auto-scaling trigger (predict â†’ scale)



5ï¸âƒ£ EVALUATION & TESTING

â”œâ”€â”€ Locust load simulation (3 scenarios: normal/spike/crash)

â”œâ”€â”€ Metrics: CPU utilization, cost/hour, response time

â”œâ”€â”€ Baseline: Traditional K8s HPA (reactive)

â””â”€â”€ Expected: 20-30% cost savings



6ï¸âƒ£ ANALYSIS & DOCUMENTATION

â”œâ”€â”€ Statistical analysis (t-test)

â”œâ”€â”€ Graphs + results tables

â”œâ”€â”€ Best practices guide

â””â”€â”€ Limitations + future work

# ğŸš€ Stage 3: ML Model Training â€“ Proactive K8s Autoscaling

This repository contains the core predictive engine for a proactive cloud autoscaling framework. Stage 3 focuses on developing a "Dual-Brain" machine learning system to forecast workload demands and detect system anomalies before they impact performance.

## ğŸ¯ Research Goal

To move cloud operations from a **reactive** model (scaling only after CPU > 80%) to a **proactive** model by predicting workload spikes **60 minutes in advance**.

## ğŸ“Š Dataset Strategy

We utilize a hybrid data approach to ensure the model is both production-ready and agile.

| Dataset | Phase Usage | Purpose | Target |
| --- | --- | --- | --- |
| **Synthetic Data** | Initial Prototype | Rapid iteration and CI/CD pipeline reliability testing. | 100% "Green" runs |
| **Google Cluster 2019** | Production Model | Training on real-world production task usage patterns. | **RMSE < 0.1** |

## ğŸ§  Model Architectures

### 1. Workload Predictor (LSTM)

* **Purpose**: Forecasts future CPU/Memory usage based on historical trends.
* **Inputs**: Past 6 time-series data points (60 minutes of history).
* **Output**: Predicted CPU demand for the next interval.
* **Success Metric**: 85% Prediction Accuracy and RMSE < 0.1.

### 2. Anomaly Detector (Isolation Forest)

* **Purpose**: Identifies sudden, non-linear spikes and potential crash scenarios (Outliers).
* **Role**: Triggers "Self-Healing" alerts or emergency recovery procedures.

## ğŸ› ï¸ Implementation Progress

* [x] **Data Preprocessing**: Normalized all CPU usage metrics to a [0, 1] range using `MinMaxScaler`.
* [x] **Initial Training (Synthetic)**: Successfully trained a prototype LSTM on 1,000 rows of synthetic data to validate the architecture.
* [ ] **Production Training (Google Cluster)**: Currently sampling 10,000 rows from the 2019 Google dataset to hit the 85% accuracy target.

## ğŸ“ˆ Expected Outcomes (Phase 5 Comparison)

Our testing aims to validate the following improvements over standard Reactive HPA:

* **Cost Savings**: 15% reduction during normal operations.
* **Downtime Reduction**: 25% faster scaling during sudden traffic spikes.
* **Recovery Speed**: 30% improvement in anomaly recovery times.

## ğŸ“‚ Repository Structure

```text
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ synthetic_traffic.csv   # Fast-iteration test data
â”‚   â””â”€â”€ google_sample.csv      # Real-world production data (10k rows)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ proactive_model_v1.h5   # Trained LSTM weights (Synthetic)
â”‚   â””â”€â”€ anomaly_detector.pkl   # Trained Isolation Forest (Prototype)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Stage3_Model_Training.ipynb  # Google Colab Training Script
â””â”€â”€ README.md

```

## ğŸ“… Timeline Status: **Day 4 of 4 (Intensive Execution)**

* **Today's Focus**: Finalizing ML accuracy benchmarks and exporting the `.h5` model for Stage 4 pipeline integration.

---

**Supervisor:** Prof. S. Vasanthapriyan | **Student:** H.M.T.N. Padiwela (20APSE4838)
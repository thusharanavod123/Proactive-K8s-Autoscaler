
---

# ðŸš€ Stage 3: ML Model Training â€“ Proactive K8s Autoscaling

This repository contains the core predictive engine for a proactive cloud autoscaling framework. Stage 3 focuses on developing a "Dual-Brain" machine learning system to forecast workload demands and detect system anomalies before they impact performance.

## ðŸŽ¯ Research Goal

To move cloud operations from a **reactive** model (scaling only after CPU > 80%) to a **proactive** model by predicting workload spikes **60 minutes in advance**.

## ðŸ“Š Dataset Strategy

We utilize a hybrid data approach to ensure the model is both production-ready and agile.

| Dataset | Phase Usage | Purpose | Target |
| --- | --- | --- | --- |
| **Synthetic Data** | Initial Prototype | Rapid iteration and CI/CD pipeline reliability testing. | 100% "Green" runs |
| **Google Cluster 2019** | Production Model | Training on real-world production task usage patterns. | **RMSE < 0.1** |

## ðŸ§  Model Architectures

### 1. Workload Predictor (LSTM)

* **Purpose**: Forecasts future CPU/Memory usage based on historical trends.
* **Inputs**: Past 6 time-series data points (60 minutes of history).
* **Output**: Predicted CPU demand for the next interval.
* **Success Metric**: 85% Prediction Accuracy and RMSE < 0.1.

### 2. Anomaly Detector (Isolation Forest)

* **Purpose**: Identifies sudden, non-linear spikes and potential crash scenarios (Outliers).
* **Role**: Triggers "Self-Healing" alerts or emergency recovery procedures.

## ðŸ› ï¸ Implementation Progress

* [x] **Data Preprocessing**: Normalized all CPU usage metrics to a [0, 1] range using `MinMaxScaler`.
* [x] **Initial Training (Synthetic)**: Successfully trained a prototype LSTM on 1,000 rows of synthetic data to validate the architecture.
* [ ] **Production Training (Google Cluster)**: Currently sampling 10,000 rows from the 2019 Google dataset to hit the 85% accuracy target.

## ðŸ“ˆ Expected Outcomes (Phase 5 Comparison)

Our testing aims to validate the following improvements over standard Reactive HPA:

* **Cost Savings**: 15% reduction during normal operations.
* **Downtime Reduction**: 25% faster scaling during sudden traffic spikes.
* **Recovery Speed**: 30% improvement in anomaly recovery times.

## ðŸ“‚ Repository Structure

```text
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ synthetic_traffic.csv   # Fast-iteration test data
â”‚   â””â”€â”€ google_sample.csv      # Real-world production data (10k rows)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ proactive_model_v1.h5   # Trained LSTM weights (Synthetic)
â”‚   â””â”€â”€ anomaly_detector.pkl   # Trained Isolation Forest (Prototype)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Stage3_Model_Training.ipynb  # Google Colab Training Script
â””â”€â”€ README.md

```

## ðŸ“… Timeline Status: **Day 4 of 4 (Intensive Execution)**

* **Today's Focus**: Finalizing ML accuracy benchmarks and exporting the `.h5` model for Stage 4 pipeline integration.

---

**Supervisor:** Prof. S. Vasanthapriyan | **Student:** H.M.T.N. Padiwela (20APSE4838)